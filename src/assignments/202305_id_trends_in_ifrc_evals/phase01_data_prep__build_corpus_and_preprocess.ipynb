{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d73acd",
   "metadata": {},
   "source": [
    "### Iterate over corpus, process PDFs by:\n",
    "1) remove formatting\n",
    "2) chunk into paragraphs and sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac977ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE SURE YOU CHANGE THIS TO BE APPROPRIATE FOR YOUR ENVIRONMENT\n",
    "REPO_ROOT=\"C://Users//ericf//source//repos//text-inferences\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8c2c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.path.realpath('.'),'..','..','common_functions'))\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eefb6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "### shouldn't need to modify these, but you will need to make sure the dirs exist\n",
    "CORPUS = os.path.join(REPO_ROOT,'corpora','ifrc_evaluations')\n",
    "INTERMEDIATES = os.path.join('.','file_intermediates')\n",
    "sys.path.append(os.path.join(REPO_ROOT,'src','common_functions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1157f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "#from sklearn.decomposition import NMF\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "#custom common functions\n",
    "import parse_pdf\n",
    "import file_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83591e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30716ef7",
   "metadata": {},
   "source": [
    "# First process the PDFs, convert to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb56feb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b52a1ac5be44ecc993a19d1632a7bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_text_corpus_from_pdfs(CORPUS):\n",
    "    # search the staged_files folder (a subdir of the corpus folder)\n",
    "    # read them in, process and output cleansed text files to corpus folder\n",
    "    staged_files = os.path.join(CORPUS,'staged_files')\n",
    "    \n",
    "    def write_text(file_content, pdf):\n",
    "        #takes in file content and name and outputs to text\n",
    "        filename = pdf.replace(\".pdf\",\".txt\")\n",
    "        full_filepath = os.path.join(CORPUS,filename)\n",
    "\n",
    "\n",
    "        with open(full_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(file_content)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # get available evaluation files\n",
    "    pdf_list = [pdf for pdf in os.listdir(staged_files) if pdf[-4:] == '.pdf']\n",
    "\n",
    "\n",
    "    text_content = \"\"\n",
    "    for pdf in tqdm(pdf_list):\n",
    "        filepath = os.path.join(staged_files,pdf)\n",
    "\n",
    "        try:\n",
    "            file_content = parse_pdf.read_pdf_document(filepath)\n",
    "            write_text(file_content, pdf)\n",
    "\n",
    "            text_content = text_content + \"\\n\" + file_content\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    \n",
    "\n",
    "generate_text_corpus_from_pdfs(CORPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2e85f3",
   "metadata": {},
   "source": [
    "## After Text Files have been generated, we can now preprocess them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f36e7ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to process 10 files.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a3627891664fb1a1cbd1e03f8644e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output has been written to 'file_outputs' to avert accidentally overwriting the working file.\n",
      "if you want to use it, make sure to move it to 'file_intermediates'\n"
     ]
    }
   ],
   "source": [
    "# paragraph level split. This function creates the csv file needed to do\n",
    "# paragraph level analysis\n",
    "\n",
    "\n",
    "def generate_paragraph_level_data_from_corpus():\n",
    "    txt_list = [txt for txt in os.listdir(CORPUS) if txt[-4:] == '.txt']\n",
    "    print(f\"about to process {len(txt_list)} files.\")\n",
    "\n",
    "\n",
    "    df_paragraph = pd.DataFrame(columns=['file','original_text'])\n",
    "\n",
    "\n",
    "    for file in tqdm(txt_list):\n",
    "\n",
    "        with open (os.path.join(CORPUS,file), \"r\", encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        paras = re.split(r'\\n\\s*\\n', content)\n",
    "\n",
    "        paragraphs = []\n",
    "\n",
    "        for p in paras:\n",
    "\n",
    "            #if the presumed paragraph is excessively long, split again into\n",
    "            #sentences and group into 5 sentences per para\n",
    "            if len(p) > 2500:\n",
    "                sents = p.split('.')\n",
    "\n",
    "                pseudo_para = ['. '.join(sents[i:i+5]) for i in range(0, len(sents), 5)]\n",
    "                for pp in pseudo_para:\n",
    "\n",
    "                    #paragraphs.append([file,pp]) if len(pp) >= 14 else next\n",
    "\n",
    "                    if len(pp) >= 14:\n",
    "                      \n",
    "                        new_row = pd.DataFrame([[file,pp]], columns=df_paragraph.columns)\n",
    "                        \n",
    "                        df_paragraph = pd.concat([df_paragraph, new_row], ignore_index=True)\n",
    "            elif len(p) < 14:\n",
    "                #paragraph is too short... just noise...\n",
    "                next\n",
    "            else:\n",
    "\n",
    "                r = [file,p]\n",
    "                new_row = pd.DataFrame([[file,p]], columns=['file','original_text'])\n",
    "                df_paragraph = pd.concat([df_paragraph, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "    df_paragraph.to_csv(\"file_outputs\\evals_by_paragraph.csv\")\n",
    "    \n",
    "    print(\"output has been written to 'file_outputs' to avert accidentally overwriting the working file.\")\n",
    "    print(\"if you want to use it, make sure to move it to 'file_intermediates'\")\n",
    "\n",
    "\n",
    "\n",
    "generate_paragraph_level_data_from_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa75a20",
   "metadata": {},
   "source": [
    "## Now Generate the preprocessed, sentence-level pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00949f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This function takes about 4 seconds per file to run. It has a list of files 10 items long.\n",
      "Expect this to take about 0.6666666666666666 minutes to complete.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6a517339f54aea97765065eea9d040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_sentence_level_preprocessed_file_for_analysis(CORPUS):\n",
    "    \n",
    "    txt_list = [txt for txt in os.listdir(CORPUS) if txt[-4:] == '.txt']\n",
    "    print(f\"This function takes about 4 seconds per file to run. It has a list of files {len(txt_list)} items long.\")\n",
    "    print(f\"Expect this to take about {len(txt_list) * 4 / 60} minutes to complete.\")\n",
    "\n",
    "    #df_paragraph = pd.DataFrame(columns=['file','paragraph'])\n",
    "    text_content = \"\"\n",
    "    df = pd.DataFrame(columns=['file','original_text','preprocessed_text'])\n",
    "\n",
    "\n",
    "    segments=[]\n",
    "    for txt in tqdm(txt_list):\n",
    "\n",
    "        filepath = os.path.join(CORPUS,txt)\n",
    "\n",
    "        try:\n",
    "            with open(filepath,\"r\", encoding=\"utf-8\") as f:\n",
    "                file_content = f.read()\n",
    "\n",
    "                doc = nlp(file_content)\n",
    "                segments = []\n",
    "\n",
    "                for sent in doc.sents:\n",
    "                    row = [txt]\n",
    "\n",
    "                    sent_after_stops = ' '.join([str(w.lemma_) for w in sent if w.is_stop == False])\n",
    "                    sent_after_stops = sent_after_stops.replace(\"\\n\",\" \")\n",
    "                    segment = simple_preprocess(sent_after_stops, deacc=True) \n",
    "                    if len(segment) > 1 :\n",
    "                        row.append(sent.text)\n",
    "                        row.append(segment)\n",
    "\n",
    "                        df.loc[len(df.index)] = row \n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    output_file = file_utils.format_outfile_name('eval_sents',extension='pkl')\n",
    "    df.to_pickle(output_file)\n",
    "    print(\"output has been written to 'file_outputs' to avert accidentally overwriting the working file.\")\n",
    "    print(\"if you want to use it, make sure to move it to 'file_intermediates' and rename accordingly.\")\n",
    "    \n",
    "generate_sentence_level_preprocessed_file_for_analysis(CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731b783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
